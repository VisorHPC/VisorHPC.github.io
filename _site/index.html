<!DOCTYPE html>
<html lang="en">
	
<!-- Basic information about this page -->
<meta charset="UTF-8">
<link rel="icon" type="image/x-icon" href="/assets/img/fav.ico">
<title>
	
	VisorHPC Workshop Website
</title>

<!-- Meta tag looking for mobile devices -->
<meta name="viewport" content="initial-scale=1.5">
<meta name="theme-color" content="#232323">

<!-- Good information about this page -->
<meta name="description" content="">
<meta name="keywords" content="virtualization, HPC">
<meta name="author" content="VisorHPC">

<!-- Feed links -->
<link rel="alternate" type="application/rss+xml" title="VisorHPC's RSS Feed" href="/rss.xml">
<link rel="alternate" type="application/atom+xml" title="VisorHPC's Atom Feed" href="/atom.xml">

<!-- Style and scripts files -->
<link rel="stylesheet" type="text/css" href="/assets/css/style.css">

<!-- Extra information about this page -->
<link rel="license" type="text/plain" src="/assets/LICENSE">





	<body>
		<header>
	<img src="../assets/img/logo.png">
</header>

		<nav>
	<center>
	<hr />
	<h1>1st Workshop on Virtualization Solutions for High-Performance Computing</h1>
	<h3>24th January 2017<h3>
	<h3>Co-located with <a href="https://www.hipeac.net/2017/stockholm/" target=_blank>HiPEAC 2017</a> in Stockholm, Sweden, January 23-25, 2017</h3>
	<hr />
	<ul>
		<li><a href="/#">Home</a></li>
		<li><a href="/#news">News</a></li>
		<li><a href="/#about">About</a></li>
		<li><a href="/#program">Program</a></li>
		<li><a href="/#topics">Topics</a></li>
		<li><a href="/#cfp">CfP</a></li>
		<li><a href="/#deadlines">Deadlines</a></li>
		<li><a href="/#committee">PC</a></li>
		<li><a href="/#contact">Contact</a></li>
	</ul>
	<hr />
	</center>
</nav>

		<main>
			<h2 id="news">News</h2>

<ul>
  <li>Jan 24, 2017: Pre-Print of <a href="proceedings.pdf">COSH/VisorHPC proceedings</a> is online.</li>
  <li>Jan 09, 2017: The (tentative) <a href="/#program">workshop program</a> is online.</li>
  <li>Jan 02, 2017: This year, we’ve decided to merge the VisorHPC workshop with the HiPEAC’s workshop on <a href="http://wwwi10.lrr.in.tum.de/~trinitic/COSH2017/">Co-Scheduling of HPC Applications</a> in order to present a comprehensive program.</li>
  <li>Dec 20, 2016: We are looking forward to the <a href="/#keynote">invited keynote talk by Prof. DK Panda</a>:</li>
</ul>
<center>
        <h3>HPC Meets Cloud: Opportunities and Challenges in Designing High-Performance MPI and Big Data Libraries on Virtualized InfiniBand Clusters</h3>
 </center>
<ul>
  <li>Dec 20, 2016: <a href="https://www.hipeac.net/2017/stockholm">Call for participation!</a> The early bird registration rate for HiPEAC 17 ends on <strong>December 25</strong>! Register right <a href="https://www.hipeac.net/2017/stockholm/register/">here</a>!</li>
  <li>Nov 30, 2016: The <a href="/#deadlines">author notification</a> has been postponed by one week to <strong>December 07</strong> (Sorry for this inconvenience, but please stay tuned!)</li>
  <li>Oct 31, 2016: The paper submission <a href="/#deadlines">deadline has been extended</a> by two weeks until <strong>November 15</strong>.</li>
  <li>Oct 21, 2016: The <a href="/#committee">program committee</a> list is online.</li>
  <li>Oct 03, 2016: It’s our great pleasure to announce that <a href="http://web.cse.ohio-state.edu/~panda/">Professor DK Panda</a> of the Ohio State University volunteered to deliver the invited keynote talk.</li>
</ul>

<h2 id="about">About VisorHPC</h2>

<p>Although virtualization solutions are quite common in server farms and cloud computing environments, their usage is in contrast by far not prevalent in the domain of high-performance computing (HPC).
This is because, at least up to now, virtualization solutions like the employment of virtual machines (VM) have proven to be too heavyweight to be acceptable in scalable HPC systems.
However, future exascale systems, equipped with a much higher degree of computing power but also with a much larger amount of computing cores than today’s HPC systems, will demand for resiliency and malleability features that may only be provided by increasing likewise the degree of virtualization within the systems.
Moreover, recent advancements, e.g. concerning container-based virtualization or regarding hardware abstraction of high-performance interconnects, currently propel the idea of introducing more virtualization solutions even in the domain of HPC.
Prominent examples for such added values stemming from virtualization and fostering resiliency, usability and malleability also in HPC systems are the possibility of live-migration and transparent checkpoint/restart, as well as the option to provide each user with an individual environment in terms of customized VM images.</p>

<h2 id="keynote">Keynote Abstract</h2>

<p>Significant growth has been witnessed during the last few years in HPC clusters with multi-/many-core processors, accelerators, and high-performance interconnects (such as InfiniBand, Omni-Path, iWARP, and RoCE).
To alleviate the cost burden, sharing HPC cluster resources to end users through virtualization for both scientific computing and Big Data processing is becoming more and more attractive.
The recently introduced Single Root I/O Virtualization (SR-IOV) technique for InfiniBand and High Speed Ethernet provides native I/O virtualization capabilities and is changing the landscape of HPC virtualization.
However, SR-IOV lacks locality-aware communication support, which leads to performance overheads for inter-VM communication even within the same host.
In this talk, we will first present our recent studies done on MVAPICH2-Virt MPI library over virtualized SR-IOV-enabled InfiniBand clusters, which can fully take advantage of SR-IOV and IVShmem to deliver near-native performance for HPC applications under Standalone, OpenStack, and Containers environments.
In the second part, we will present a framework for extending SLURM with virtualization-oriented capabilities, such as dynamic virtual machine creation with SR-IOV and IVShmem resources, to effectively run MPI jobs over virtualized InfiniBand clusters.
Next, we will demonstrate how high-performance solutions can be designed to run Big Data applications (like Hadoop) in HPC cloud environments. Finally, we will share our experiences of running these designs on the <a href="https://www.chameleoncloud.org">Chameleon Cloud</a> testbed.</p>

<p><strong>The keynote will be delivered by Dhabaleswar K. (DK) Panda of The Ohio State University.</strong></p>

<p>DK Panda is a Professor and University Distinguished Scholar of Computer Science and Engineering at the Ohio State University.
He has published over 400 papers in the area of high-end computing and networking.
The <a href="http://mvapich.cse.ohio-state.edu">MVAPICH2</a> (High Performance MPI and PGAS over InfiniBand, Omni-Path, iWARP and RoCE) libraries, designed and developed by his research group, are currently being used by more than 2,700 organizations worldwide (in 83 countries).
More than 405,000 downloads of this software have taken place from the project’s site.
As of Nov’16, this software is empowering several InfiniBand clusters (including the 1st, 13th, 17th, and 40th ranked ones) in the TOP500 list.
The RDMA packages for Apache Spark, Apache Hadoop, Apache HBase, and Memcached together with OSU HiBD benchmarks from his group are also <a href="http://hibd.cse.ohio-state.edu">publicly available</a>.
These libraries are currently being used by more than 200 organizations in 29 countries.
More than 19,000 downloads of these libraries have taken place.
He is an IEEE Fellow.</p>

<p>More details about Prof. Panda are available at <a href="http://www.cse.ohio-state.edu/~panda">http://www.cse.ohio-state.edu/~panda</a>.</p>

<h2 id="program">Program</h2>

<ul>
  <li>
    <p>10:00 - 10:10 Welcome and Introduction</p>
  </li>
  <li>
    <p>10:10 - 11:10 Keynote Talk by Prof. DK Panda:<br />
 <a href="slides/dk_keynote_visorhpc17.pdf"> <strong>HPC Meets Cloud: Opportunities and Challenges in Designing High-Performance MPI and Big Data Libraries on Virtualized InfiniBand Clusters</strong></a></p>
  </li>
  <li>
    <p>Coffee Break</p>
  </li>
  <li>
    <p>11:40 - 12:00 S. Fan, F. Chen, H. Rauchfuss, N. Har’el, U. Schilling and N. Struckmann:<br />
<strong>Towards a Lightweight RDMA Para-Virtualization for HPC</strong></p>
  </li>
  <li>
    <p>12:00 - 12:30 T. Küstner, C. Trinitis, J. Weidendorfer, A. Blaszczyk, P. Kaufmann and M. Johansson:<br />
<a href="slides/jw_visorhpc17.pdf"><strong>On the Applicability of Virtualization in an Industrial HPC Environment</strong></a></p>
  </li>
  <li>
    <p>12:30 - 13:00 S. Pickartz, J. Breitbart and S. Lankes:<br />
<a href="slides/sl_cosch17.pdf"><strong>Co-scheduling on Upcoming Many-Core Architectures</strong></a></p>
  </li>
  <li>
    <p>Lunch Break</p>
  </li>
  <li>
    <p>14:15 - 15:00 Invited Talk by Dr. Mikko Byckling from Intel Finland:<br />
<strong>An Update on Intel Technologies for High Performance Computing</strong></p>
  </li>
  <li>
    <p>15:00 - 15:30 A. de Blanche and T. Lundqvist:<br />
<strong>Disallowing Same-program Co-schedules to Improve Efficiency in Quad-core Servers</strong></p>
  </li>
  <li>
    <p>Coffee Break</p>
  </li>
  <li>
    <p>16:00 - 16:30 I. Papadakis, K. Nikas, V. Karakostas, G. Goumas and N. Koziris:<br />
<strong>Improving QoS and Utilisation in modern multi-core servers with Dynamic Cache Partitioning</strong></p>
  </li>
  <li>
    <p>16:30 - 16:45 HermitCore Live Demo by Dr. Stefan Lankes:<br />
<a href="slides/sl_hermit17.pdf"><strong>HermitCore: A Library Operating System for Cloud and High-Performance Computing</strong></a></p>
  </li>
  <li>
    <p>16:45 - 17:00 Wrap-up and Workshop Closing</p>
  </li>
</ul>

<h2 id="topics">Workshop Topics</h2>

<p><strong><em>All</em></strong> <strong>subjects concerning virtualization solutions in HPC</strong>, especially (but not limited to):</p>

<ul>
  <li>Employment and management of virtual machines and/or software containers in HPC</li>
  <li>Checkpointing and/or migration solutions for resiliency and malleability in HPC</li>
  <li>Solutions for network and I/O virtualization regarding HPC interconnects</li>
  <li>Parallel I/O and HPC storage solutions taking advantage from virtualization</li>
  <li>Hypervisors and other virtualization solutions tailored for HPC systems</li>
  <li>Virtualization solutions for dealing with heterogeneity in HPC environments</li>
  <li>Extensions to resource managers and job schedulers with respect to virtualization</li>
  <li>Adaptation of cloud-computing technologies to HPC environments</li>
  <li>Operating system support for virtualization in HPC systems</li>
  <li>Performance and power analysis of virtualized HPC systems</li>
  <li>Debugging and/or profiling in virtual environments</li>
</ul>

<h2 id="cfp">Call for Papers</h2>

<ul>
  <li>Open call for papers</li>
  <li>All papers will be peer-reviewed</li>
  <li>Accepted papers will be published online</li>
  <li>Proceedings will be referenceable through ISBN plus DOI</li>
  <li>Three types of papers are welcome for submission:
    <ul>
      <li>Full (regular) research papers of 6-8 pages (will be peer-reviewed by at least 3 reviewers)</li>
      <li>Short papers of up to 4 pages (may include content not yet mature enough for a full paper)</li>
      <li>Industrial white papers of up to 4 pages (may present products from a technical point of view)</li>
    </ul>
  </li>
</ul>

<h2 id="submission-guidelines">Submission Guidelines</h2>

<ul>
  <li>Papers should be formatted according to the <a href="http://www.ieee.org/conferences_events/conferences/publishing/templates.html">IEEE manuscript templates</a> for conferences</li>
  <li>Please stick to the page limits of 4 and 8 pages, respectively</li>
  <li>Submissions should be done via the <a href="https://easychair.org/conferences/?conf=visorhpc2017">EasyChair submission server</a></li>
</ul>

<h2 id="deadlines">Preliminary Deadlines</h2>

<ul>
  <li>Paper submission deadline:		<del>October 31</del> November 15, 2016</li>
  <li>Notification of acceptance:		<del>November 30</del> December 07, 2016</li>
  <li>Camera ready due:			<del>December 15</del> January 15, 2016</li>
</ul>

<h2 id="committee">Program Committee</h2>
<ul>
  <li>Frank Bellosa, Karlsruhe Institute of Technology (KIT)</li>
  <li>Carsten Clauss, ParTec Cluster Competence Center GmbH</li>
  <li>Stefan Lankes, RWTH Aachen University</li>
  <li>Julián Morillo Pozo, Barcelona Supercomputing Center (BSC)</li>
  <li>Lena Oden, Argonne National Laboratory (ANL)</li>
  <li>Pablo Reble, Intel Corporation</li>
  <li>Josh Simons, VMware, Inc.</li>
  <li>Josef Weidendorfer, Technical University of Munich (TUM)</li>
</ul>

<h2 id="contact">Organizers and Contact</h2>

<ul>
  <li>Carsten Clauss, ParTec Cluster Competence Center GmbH, Germany, Email: <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#099;&#108;&#097;&#117;&#115;&#115;&#064;&#112;&#097;&#114;&#045;&#116;&#101;&#099;&#046;&#099;&#111;&#109;">&#099;&#108;&#097;&#117;&#115;&#115;&#064;&#112;&#097;&#114;&#045;&#116;&#101;&#099;&#046;&#099;&#111;&#109;</a></li>
  <li>Stefan Lankes, RWTH Aachen University, Germany, Email: <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#115;&#108;&#097;&#110;&#107;&#101;&#115;&#064;&#101;&#111;&#110;&#101;&#114;&#099;&#046;&#114;&#119;&#116;&#104;&#045;&#097;&#097;&#099;&#104;&#101;&#110;&#046;&#100;&#101;">&#115;&#108;&#097;&#110;&#107;&#101;&#115;&#064;&#101;&#111;&#110;&#101;&#114;&#099;&#046;&#114;&#119;&#116;&#104;&#045;&#097;&#097;&#099;&#104;&#101;&#110;&#046;&#100;&#101;</a></li>
</ul>

		</main>
		<footer>
<hr />
Copyright 2016
<hr />
</footer>

	</body>
</html>
